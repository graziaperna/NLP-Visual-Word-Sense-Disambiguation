{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "dxoEEdu-BCYs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#%pip install -U spaCy\n",
        "#%pip install keras\n",
        "#%python -m spacy download en_core_web_sm\n",
        "#%pip install -U jax jaxlib\n",
        "#%pip install clean-text\n",
        "#%pip install --upgrade ipykernel\n",
        "#%pip install tensorflow\n",
        "#%pip install numpy --upgrade\n",
        "#%pip install numpy==1.21.6\n",
        "#%pip uninstall tensorflow\n",
        "#da eseguire al primo utilizzo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Wel4vXK2BCYu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of CPUs in the system: 8\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import spacy\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from spacy.lang.it.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from pandas import read_csv\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from sklearn import neural_network\n",
        "from sklearn import ensemble\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from pandas import read_csv\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "#init spaCy\n",
        "punctuations = string.punctuation\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "parser = English()\n",
        "\n",
        "# Custom transformer using spaCy\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    text_without_punctuation = text.translate(translator)\n",
        "    return text_without_punctuation.lower()\n",
        "    #return text.strip().lower()\n",
        "\n",
        "# Tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    mytokens = parser(sentence)\n",
        "    mytokens = [ word.text for word in mytokens ]\n",
        "    # remove stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "  \n",
        "n_cpu = os.cpu_count()\n",
        "print(\"Number of CPUs in the system:\", n_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmyyylg4Cg3U",
        "outputId": "ceed22cd-c394-41cf-b576-fd892f5ed982"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "SvR_yoZ3BCYv",
        "outputId": "7acb3bfe-bf2d-4d39-a9b1-0127641e04b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN: \n",
            " not sexist    10602\n",
            "sexist         3398\n",
            "Name: label_sexist, dtype: int64\n",
            "\n",
            "VALIDATION: \n",
            " not sexist    1514\n",
            "sexist         486\n",
            "Name: label_sexist, dtype: int64\n",
            "\n",
            "TEST: \n",
            " not sexist    3030\n",
            "sexist         970\n",
            "Name: label_sexist, dtype: int64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dft = read_csv('../Dataset/Processed datasets/train_sexist.csv')\n",
        "dfv = read_csv('../Dataset/Processed datasets/dev_sexist.csv')\n",
        "dfs = read_csv('../Dataset/Processed datasets/test_sexist.csv')\n",
        "\n",
        "x_train = dft['text']\n",
        "y_train = dft['label_sexist']\n",
        "x_val = dfv['text']\n",
        "y_val = dfv['label_sexist']\n",
        "x_test = dfs['text']\n",
        "y_test= dfs['label_sexist']\n",
        "dft.set_index('ID')\n",
        "dfv.set_index('ID')\n",
        "dfs.set_index('ID')\n",
        "print(\"TRAIN: \\n\", y_train.value_counts(), end=\"\\n\\n\")\n",
        "print(\"VALIDATION: \\n\", y_val.value_counts(), end=\"\\n\\n\")\n",
        "print(\"TEST: \\n\", y_test.value_counts(), end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CoUpof7FFfS9"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "tokenizer1 = MWETokenizer()\n",
        "tokenizer2 = TreebankWordTokenizer()\n",
        "vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
        "vector6 = CountVectorizer(tokenizer = tokenizer2.tokenize, ngram_range=(1,2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Task A**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x0000028D25F7B820>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(ngram_range=(1, 2),\n",
              "                                 tokenizer=<bound method TreebankWordTokenizer.tokenize of <nltk.tokenize.treebank.TreebankWordTokenizer object at 0x0000028D267923A0>>)),\n",
              "                ('classifier',\n",
              "                 LinearSVC(class_weight={'not sexist': 0.2, 'sexist': 1},\n",
              "                           max_iter=10000))])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.utils import class_weight\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, precision_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "classifier = svm.LinearSVC(max_iter = 10000, class_weight= {\"not sexist\": 0.2, \"sexist\": 1})\n",
        "# Multi-layer Perceptron classifier\n",
        "#classifier = neural_network.MLPClassifier(hidden_layer_sizes=(512,),verbose=True,max_iter=10)\n",
        "# Random Foreset\n",
        "#classifier = ensemble.RandomForestClassifier(class_weight='balanced')\n",
        "\n",
        "# Create the pipeline\n",
        "\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "('vectorizer', vector6),\n",
        "('classifier', classifier)])\n",
        "\n",
        "pipe.fit(x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8265\n",
            "Validation Precision: 0.7770\n",
            "Validation Recall: 0.7150\n",
            "Validation F1 Score: 0.7365\n"
          ]
        }
      ],
      "source": [
        "y_pred = pipe.predict(x_val)\n",
        "\n",
        "# Calcolo delle metriche di valutazione sul set di test\n",
        "val_accuracy = accuracy_score(y_val, y_pred)\n",
        "val_precision = precision_score(y_val, y_pred, average=\"macro\")\n",
        "val_recall = recall_score(y_val, y_pred, average=\"macro\")\n",
        "val_f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
        "\n",
        "# Print the evaluation metrics for the validation data\n",
        "print(\"Validation Accuracy: {:.4f}\".format(val_accuracy))\n",
        "print(\"Validation Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Validation Recall: {:.4f}\".format(val_recall))\n",
        "print(\"Validation F1 Score: {:.4f}\".format(val_f1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Migliori iperparametri: {'classifier__C': 0.2, 'classifier__class_weight': 'balanced', 'vectorizer__ngram_range': (1, 2)}\n"
          ]
        }
      ],
      "source": [
        "params = {'vectorizer__ngram_range': [(1,1),(1,2)],\n",
        "          'classifier__C': [0.2,0.4],\n",
        "          'classifier__class_weight': [{\"not sexist\": 0.7, \"sexist\": 1}, 'balanced', None]}\n",
        "\n",
        "pipe_optimized = GridSearchCV(pipe,param_grid=params,cv=10,scoring=make_scorer(f1_score, average=\"macro\"),n_jobs=n_cpu-1,refit=True)\n",
        "pipe_optimized.fit(x_train, y_train)\n",
        "print(\"Migliori iperparametri:\",pipe_optimized.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x0000028D25F7B820>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(ngram_range=(1, 2),\n",
              "                                 tokenizer=<bound method TreebankWordTokenizer.tokenize of <nltk.tokenize.treebank.TreebankWordTokenizer object at 0x0000028D267923A0>>)),\n",
              "                ('classifier',\n",
              "                 LinearSVC(C=0.2, class_weight='balanced', max_iter=10000))])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = pipe_optimized.best_params_\n",
        "pipe.set_params(**best_params)\n",
        "pipe.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8397\n",
            "Validation Precision: 0.7944\n",
            "Validation Recall: 0.7414\n",
            "Validation F1 Score: 0.7615\n"
          ]
        }
      ],
      "source": [
        "y_pred = pipe_optimized.predict(x_test)\n",
        "\n",
        "# Calcolo delle metriche di valutazione sul set di test\n",
        "\n",
        "val_accuracy = accuracy_score(y_test, y_pred)\n",
        "val_precision = precision_score(y_test, y_pred, average=\"macro\")\n",
        "val_recall = recall_score(y_test, y_pred, average=\"macro\")\n",
        "val_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "# Print the evaluation metrics for the validation data\n",
        "\n",
        "print(\"Validation Accuracy: {:.4f}\".format(val_accuracy))\n",
        "print(\"Validation Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Validation Recall: {:.4f}\".format(val_recall))\n",
        "print(\"Validation F1 Score: {:.4f}\".format(val_f1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Task B**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN: \n",
            " 2. derogation                               1590\n",
            "3. animosity                                1165\n",
            "4. prejudiced discussions                    333\n",
            "1. threats, plans to harm and incitement     310\n",
            "Name: label_category, dtype: int64\n",
            "\n",
            "VALIDATION: \n",
            " 2. derogation                               227\n",
            "3. animosity                                167\n",
            "4. prejudiced discussions                    48\n",
            "1. threats, plans to harm and incitement     44\n",
            "Name: label_category, dtype: int64\n",
            "\n",
            "TEST: \n",
            " 2. derogation                               454\n",
            "3. animosity                                333\n",
            "4. prejudiced discussions                    94\n",
            "1. threats, plans to harm and incitement     89\n",
            "Name: label_category, dtype: int64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dft = read_csv('../Dataset/Processed datasets/train_category.csv')\n",
        "dfv = read_csv('../Dataset/Processed datasets/dev_category.csv')\n",
        "dfs = read_csv('../Dataset/Processed datasets/test_category.csv')\n",
        "\n",
        "x1_train = dft['text']\n",
        "y1_train = dft['label_category']\n",
        "x1_val = dfv['text']\n",
        "y1_val = dfv['label_category']\n",
        "x1_test = dfs['text']\n",
        "y1_test= dfs['label_category']\n",
        "dft.set_index('ID')\n",
        "dfv.set_index('ID')\n",
        "dfs.set_index('ID')\n",
        "print(\"TRAIN: \\n\", y1_train.value_counts(), end=\"\\n\\n\")\n",
        "print(\"VALIDATION: \\n\", y1_val.value_counts(), end=\"\\n\\n\")\n",
        "print(\"TEST: \\n\", y1_test.value_counts(), end=\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "def treebankWordTokenizer(sentence):\n",
        "\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "    # Lemmatize each token\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'kerastokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[55], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m vector5 \u001b[39m=\u001b[39m CountVectorizer(tokenizer \u001b[39m=\u001b[39m tokenizer1\u001b[39m.\u001b[39mtokenize, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m5\u001b[39m))\n\u001b[0;32m     15\u001b[0m vector6 \u001b[39m=\u001b[39m CountVectorizer(tokenizer \u001b[39m=\u001b[39m treebankWordTokenizer, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[1;32m---> 17\u001b[0m vector7 \u001b[39m=\u001b[39m CountVectorizer(tokenizer \u001b[39m=\u001b[39m kerastokenizer, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[0;32m     19\u001b[0m vector8 \u001b[39m=\u001b[39m TfidfVectorizer(tokenizer \u001b[39m=\u001b[39m kerastokenizer, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'kerastokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "tokenizer1 = MWETokenizer()\n",
        "tokenizer2 = TreebankWordTokenizer()\n",
        "\n",
        "vector1 = CountVectorizer(tokenizer = tokenizer.tokenize, ngram_range=(1,2))\n",
        "vector2 = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2))\n",
        "vector3 = TfidfVectorizer(tokenizer = tokenizer.tokenize, ngram_range=(1,2))\n",
        "vector4 = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2))\n",
        "\n",
        "vector5 = CountVectorizer(tokenizer = tokenizer1.tokenize, ngram_range=(1,5))\n",
        "vector6 = CountVectorizer(tokenizer = treebankWordTokenizer, ngram_range=(1,2))\n",
        "\n",
        "vector7 = CountVectorizer(tokenizer = kerastokenizer, ngram_range=(1,2))\n",
        "\n",
        "vector8 = TfidfVectorizer(tokenizer = kerastokenizer, ngram_range=(1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x0000028D4E9EBE50>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(ngram_range=(1, 2),\n",
              "                                 tokenizer=<function treebankWordTokenizer at 0x0000028D3504BF70>)),\n",
              "                ('classifier',\n",
              "                 LinearSVC(class_weight='balanced', max_iter=10000))])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.utils import class_weight\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, precision_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "classifier = svm.LinearSVC(max_iter = 10000, class_weight= 'balanced')\n",
        "# Multi-layer Perceptron classifier\n",
        "#classifier = neural_network.MLPClassifier(hidden_layer_sizes=(200,),verbose=True,max_iter=1000, shuffle = True, tol = 0.0001,warm_start = True)\n",
        "# Random Foreset\n",
        "#classifier = ensemble.RandomForestClassifier(class_weight='balanced',n_estimators=300,criterion='entropy',warm_start=True)\n",
        "\n",
        "# Create the pipeline\n",
        "\n",
        "pipe_cat = Pipeline([(\"cleaner\", predictors()),\n",
        "('vectorizer', vector7),\n",
        "('classifier', classifier)])\n",
        "\n",
        "pipe_cat.fit(x1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.5370\n",
            "Validation Precision: 0.4971\n",
            "Validation Recall: 0.4182\n",
            "Validation F1 Score: 0.4348\n"
          ]
        }
      ],
      "source": [
        "y_pred = pipe_cat.predict(x1_val)\n",
        "\n",
        "# Calcolo delle metriche di valutazione sul set di test\n",
        "val_accuracy = accuracy_score(y1_val, y_pred)\n",
        "val_precision = precision_score(y1_val, y_pred, average=\"macro\")\n",
        "val_recall = recall_score(y1_val, y_pred, average=\"macro\")\n",
        "val_f1 = f1_score(y1_val, y_pred, average=\"macro\")\n",
        "\n",
        "# Print the evaluation metrics for the validation data\n",
        "print(\"Validation Accuracy: {:.4f}\".format(val_accuracy))\n",
        "print(\"Validation Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Validation Recall: {:.4f}\".format(val_recall))\n",
        "print(\"Validation F1 Score: {:.4f}\".format(val_f1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Migliori iperparametri: {'classifier__class_weight': 'balanced', 'vectorizer__ngram_range': (1, 2)}\n"
          ]
        }
      ],
      "source": [
        "params = {'vectorizer__ngram_range': [(1,1),(1,2)],\n",
        "          'classifier__class_weight': ['balanced', None]}\n",
        "\n",
        "pipe1_optimized = GridSearchCV(pipe_cat,param_grid=params,cv=10,scoring=make_scorer(f1_score, average=\"macro\"),n_jobs=n_cpu-1,refit=True)\n",
        "pipe1_optimized.fit(x1_train, y1_train)\n",
        "print(\"Migliori iperparametri:\",pipe1_optimized.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x0000028D4E9EBE50>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(ngram_range=(1, 2),\n",
              "                                 tokenizer=<function treebankWordTokenizer at 0x0000028D3504BF70>)),\n",
              "                ('classifier',\n",
              "                 LinearSVC(class_weight='balanced', max_iter=10000))])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = pipe1_optimized.best_params_\n",
        "pipe_cat.set_params(**best_params)\n",
        "pipe_cat.fit(x1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.5052\n",
            "Validation Precision: 0.4984\n",
            "Validation Recall: 0.4235\n",
            "Validation F1 Score: 0.4455\n"
          ]
        }
      ],
      "source": [
        "y_pred = pipe_cat.predict(x1_test)\n",
        "\n",
        "# Calcolo delle metriche di valutazione sul set di test\n",
        "val_accuracy = accuracy_score(y1_test, y_pred)\n",
        "val_precision = precision_score(y1_test, y_pred, average=\"macro\")\n",
        "val_recall = recall_score(y1_test, y_pred, average=\"macro\")\n",
        "val_f1 = f1_score(y1_test, y_pred, average=\"macro\")\n",
        "\n",
        "# Print the evaluation metrics for the validation data\n",
        "print(\"Validation Accuracy: {:.4f}\".format(val_accuracy))\n",
        "print(\"Validation Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Validation Recall: {:.4f}\".format(val_recall))\n",
        "print(\"Validation F1 Score: {:.4f}\".format(val_f1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
